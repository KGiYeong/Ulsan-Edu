import streamlit as st

import os

from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

from langchain_community.document_loaders import PyPDFLoader

from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_community.vectorstores import FAISS



# 1. í˜ì´ì§€ ì„¤ì •

st.set_page_config(page_title="PDF í…ŒìŠ¤íŠ¸ ì±—ë´‡", page_icon="ğŸ¤–")

st.title("ğŸ“„ GitHub íŒŒì¼ ì½ê¸° í…ŒìŠ¤íŠ¸")



# API í‚¤ ì„¤ì •

if "GEMINI_API_KEY" not in st.secrets:

    st.error("âš ï¸ Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")

    st.stop()



os.environ["GOOGLE_API_KEY"] = st.secrets["GEMINI_API_KEY"]



# 2. PDF ë¡œë“œ ë° í•™ìŠµ

@st.cache_resource

def load_pdf_and_make_bot():

    file_path = "test.pdf"

    

    # íŒŒì¼ ì¡´ì¬ í™•ì¸

    if not os.path.exists(file_path):

        st.error(f"âŒ '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        st.info("GitHub ì €ì¥ì†Œì— test.pdf íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

        return None

    

    try:

        st.info("ğŸ“„ PDF íŒŒì¼ì„ ì½ëŠ” ì¤‘...")

        loader = PyPDFLoader(file_path)

        docs = loader.load()

        st.success(f"âœ… PDF ë¡œë“œ ì™„ë£Œ: {len(docs)}í˜ì´ì§€")

        

        st.info("âœ‚ï¸ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ëŠ” ì¤‘...")

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

        splits = text_splitter.split_documents(docs)

        st.success(f"âœ… í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(splits)}ê°œ ì¡°ê°")

        

        st.info("ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

        # ì„ë² ë”© ëª¨ë¸ - models/text-embedding-004 ì‚¬ìš©

        embeddings = GoogleGenerativeAIEmbeddings(

            model="models/text-embedding-004",

            task_type="retrieval_document"

        )

        

        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)

        st.success("âœ… í•™ìŠµ ì™„ë£Œ!")

        

        return vectorstore.as_retriever()

        

    except Exception as e:

        st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        st.info("ğŸ’¡ Gemini API í• ë‹¹ëŸ‰ì„ í™•ì¸í•˜ê±°ë‚˜, ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

        return None



retriever = load_pdf_and_make_bot()



if retriever is None:

    st.warning("âš ï¸ ì±—ë´‡ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìœ„ì˜ ì˜¤ë¥˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    st.stop()



# 3. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤

if "messages" not in st.session_state:

    st.session_state.messages = []



for message in st.session_state.messages:

    with st.chat_message(message["role"]):

        st.markdown(message["content"])



if prompt := st.chat_input("test.pdf ë‚´ìš©ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”!"):

    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user"):

        st.markdown(prompt)

    

    with st.chat_message("assistant"):

        try:

            with st.spinner("ë‹µë³€ì„ ì°¾ëŠ” ì¤‘..."):

                docs = retriever.invoke(prompt)

                context = "\n\n".join([doc.page_content for doc in docs])

                

                llm = ChatGoogleGenerativeAI(

                    model="gemini-2.5-flash",

                    temperature=0

                )

                

                full_prompt = f"""ë‹¤ìŒ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 

ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ë‹¤ë©´ 'ì£„ì†¡í•©ë‹ˆë‹¤. í•™êµ ê³µì§€ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
