import streamlit as st
import os
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ìš°ë¦¬í•™êµ ì•Œë¦¼ì´ ì±—ë´‡", page_icon="ğŸ«")
st.title("ğŸ« í•™êµ ì†Œì‹ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")
st.info("ê³µì§€ì‚¬í•­ì´ë‚˜ ê°€ì •í†µì‹ ë¬¸ PDF íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•˜ë©´ ì±—ë´‡ì´ ë‚´ìš©ì„ í•™ìŠµí•©ë‹ˆë‹¤.")

# API í‚¤ ì„¤ì •
if "GEMINI_API_KEY" not in st.secrets:
    st.error("Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()
os.environ["GOOGLE_API_KEY"] = st.secrets["GEMINI_API_KEY"]

# 2. íŒŒì¼ ì—…ë¡œë“œ ë° ë²¡í„° DB ìƒì„± í•¨ìˆ˜
def create_vector_db(uploaded_files):
    # ì„ì‹œ ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì €ì¥
    if not os.path.exists("temp_docs"):
        os.makedirs("temp_docs")
    
    all_documents = []
    for uploaded_file in uploaded_files:
        file_path = os.path.join("temp_docs", uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # PDF ë¡œë“œ
        loader = PyPDFLoader(file_path)
        all_documents.extend(loader.load())

    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_documents(all_documents)

    # ë²¡í„° ì €ì¥ì†Œ ìƒì„± (gemini-2.5-flashì™€ í˜¸í™˜ë˜ëŠ” ì„ë² ë”©)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
    return vectorstore

# ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ ì—…ë¡œë“œ
with st.sidebar:
    st.header("íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader("PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)", type="pdf", accept_multiple_files=True)
    process_button = st.button("í•™ìŠµ ì‹œì‘")

if process_button and uploaded_files:
    with st.spinner("í•™êµ ì†Œì‹ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!"):
        st.session_state.vector_db = create_vector_db(uploaded_files)
        st.success("ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ì§ˆë¬¸ì„ ì‹œì‘í•˜ì„¸ìš”.")

# 3. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ì˜ˆ: ì´ë²ˆì£¼ ì¤€ë¹„ë¬¼ì´ ë­ì•¼?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if "vector_db" not in st.session_state:
            response = "ë¨¼ì € ì™¼ìª½ì—ì„œ í•™êµ ì„œë¥˜(PDF)ë¥¼ ì—…ë¡œë“œí•˜ê³  'í•™ìŠµ ì‹œì‘'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!"
        else:
            # RAG ì²´ì¸ êµ¬ì„± (ìµœì‹  ëª¨ë¸ gemini-2.5-flash ì‚¬ìš©)
            llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)
            
            # ê²€ìƒ‰ ë° ë‹µë³€
            retriever = st.session_state.vector_db.as_retriever()
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True
            )
            
            # ë‹µë³€ ìƒì„± ì‹œ í”„ë¡¬í”„íŠ¸ ë³´ê°• (ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  í•˜ê¸°)
            result = qa_chain({"query": f"{prompt} (ë§Œì•½ ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ë‹¤ë©´ 'í•™êµì—ì„œ ì•ˆë‚´ëœ ë°”ê°€ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•´ì¤˜)"})
            response = result["result"]
            
        st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
