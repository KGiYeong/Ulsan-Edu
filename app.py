import streamlit as st
import os
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="PDF ì±—ë´‡", page_icon="ğŸ¤–")
st.title("ğŸ“„ í•™êµ ê³µì§€ì‚¬í•­ ì±—ë´‡")

# API í‚¤ ì„¤ì •
if "GEMINI_API_KEY" not in st.secrets:
    st.error("âš ï¸ Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
    st.stop()

os.environ["GOOGLE_API_KEY"] = st.secrets["GEMINI_API_KEY"]

# 2. PDF ë¡œë“œ ë° í•™ìŠµ (ìƒíƒœ ë©”ì‹œì§€ ì œê±°)
@st.cache_resource
def load_pdf_and_make_bot():
    file_path = "test.pdf"
    
    if not os.path.exists(file_path):
        return None
    
    try:
        # ì¡°ìš©íˆ ë¡œë“œ ë° ë¶„í• 
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        splits = text_splitter.split_documents(docs)
        
        # ì„ë² ë”© ìƒì„±
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            task_type="retrieval_document"
        )
        
        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
        return vectorstore.as_retriever()
        
    except Exception:
        return None

# ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë™ì•ˆ í™”ë©´ì— ì•„ë¬´ê²ƒë„ ë„ìš°ì§€ ì•Šê±°ë‚˜ ì•„ì£¼ ì§§ê²Œ ëŒ€ê¸°
retriever = load_pdf_and_make_bot()

if retriever is None:
    st.error("âŒ PDFë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'test.pdf' íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# 3. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì§ˆë¬¸ ì…ë ¥
if prompt := st.chat_input("ê³µì§€ì‚¬í•­ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        try:
            with st.spinner("ìƒê° ì¤‘..."): # ìµœì†Œí•œì˜ ë¡œë”© í‘œì‹œ
                docs = retriever.invoke(prompt)
                context = "\n\n".join([doc.page_content for doc in docs])
                
                llm = ChatGoogleGenerativeAI(
                    model="gemini-1.5-flash", # ë˜ëŠ” "gemini-2.0-flash"
                    temperature=0
                )
                
                full_prompt = f"""ë‹¤ìŒ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 
ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ë‹¤ë©´ 'ì£„ì†¡í•©ë‹ˆë‹¤. í•™êµ ê³µì§€ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {prompt}

ë‹µë³€:"""
                
                response = llm.invoke(full_prompt).content
                st.markdown(response)
                
        except Exception as e:
            response = "âŒ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            st.error(response)
    
    st.session_state.messages.append({"role": "assistant", "content": response})
