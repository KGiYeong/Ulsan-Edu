import streamlit as st
import os
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="PDF í…ŒìŠ¤íŠ¸ ì±—ë´‡", page_icon="ğŸ¤–")
st.title("ğŸ“„ GitHub íŒŒì¼ ì½ê¸° í…ŒìŠ¤íŠ¸")

# API í‚¤ ì„¤ì • (Streamlit Secrets í•„ìˆ˜)
if "GEMINI_API_KEY" not in st.secrets:
    st.error("ì„¤ì •(Secrets)ì— 'GEMINI_API_KEY'ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
    st.stop()

os.environ["GOOGLE_API_KEY"] = st.secrets["GEMINI_API_KEY"]

# 2. PDF ë¡œë“œ ë° í•™ìŠµ (test.pdf ì „ìš©)
@st.cache_resource
def load_pdf_and_make_bot():
    file_path = "test.pdf"  # GitHubì— ì˜¬ë¦° íŒŒì¼ ì´ë¦„
    
    if not os.path.exists(file_path):
        st.error(f"'{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GitHubì— íŒŒì¼ì„ ì˜¬ë ¸ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    
    # PDF ì½ê¸°
    loader = PyPDFLoader(file_path)
    docs = loader.load()
    
    # í…ìŠ¤íŠ¸ ë‚˜ëˆ„ê¸°
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_documents(docs)
    
    # ì„ë² ë”©(ê³µë¶€í•˜ê¸°) ë° ì €ì¥ì†Œ ë§Œë“¤ê¸°
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
    
    return vectorstore.as_retriever()

# ì±—ë´‡ ì¤€ë¹„
retriever = load_pdf_and_make_bot()

# 3. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("test.pdf ë‚´ìš©ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”!"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        if retriever is None:
            response = "íŒŒì¼ì´ ì—†ì–´ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            # LLM ì´ˆê¸°í™”
            llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp", temperature=0)
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
            prompt_template = ChatPromptTemplate.from_template(
                """ë‹¤ìŒ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 
                ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ë‹¤ë©´ 'ì£„ì†¡í•©ë‹ˆë‹¤. í•™êµ ê³µì§€ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
                
                ë¬¸ì„œ ë‚´ìš©:
                {context}
                
                ì§ˆë¬¸: {input}
                
                ë‹µë³€:"""
            )
            
            # ì²´ì¸ ìƒì„±
            document_chain = create_stuff_documents_chain(llm, prompt_template)
            retrieval_chain = create_retrieval_chain(retriever, document_chain)
            
            # ë‹µë³€ ìƒì„±
            with st.spinner("ë‹µë³€ì„ ì°¾ëŠ” ì¤‘..."):
                result = retrieval_chain.invoke({"input": prompt})
                response = result["answer"]
                st.markdown(response)
    
    st.session_state.messages.append({"role": "assistant", "content": response})
```

**ì£¼ìš” ë³€ê²½ì‚¬í•­:**

1. **RetrievalQA ëŒ€ì‹  create_retrieval_chain ì‚¬ìš©**: ìµœì‹  LangChain ë°©ì‹
2. **í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ê°€**: ëª…í™•í•œ ì§€ì‹œì‚¬í•­ ì „ë‹¬
3. **ëª¨ë¸ëª… ìˆ˜ì •**: `gemini-2.5-flash` â†’ `gemini-2.0-flash-exp` (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ëª…)

**requirements.txtì— ë‹¤ìŒ íŒ¨í‚¤ì§€ë“¤ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:**
```
streamlit
langchain
langchain-google-genai
langchain-community
faiss-cpu
pypdf
